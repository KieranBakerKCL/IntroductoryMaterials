{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0f0b76",
   "metadata": {},
   "source": [
    "# Statistical Modelling Brown Bag\n",
    "\n",
    "Welcome to the Statistical Modelling brown bag session, where you will be introduced to some of the key concepts we use on a day-to-day basis, and help you think about data in a similar way to us. Run the notebook as the session goes along to see all of this working in real time from your own laptop.\n",
    "\n",
    "#### Some Jupyter Notebook Instructions\n",
    "To run a cell within jupyter notebooks, you can click on the cell and press the run button at the top of the window, or you can use \n",
    "<kbd>Ctrl</kbd>+<kbd>Enter</kbd>. Also <kbd>Shift</kbd>+<kbd>Enter</kbd> will also run the cell and move you on to the next cell. \n",
    "\n",
    "Test this out on the cell below, once you have run it, you will see the words \"Hello World\" appear underneath the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47770e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2777d28",
   "metadata": {},
   "source": [
    "### Importing packages\n",
    "\n",
    "Now run the cell below to import the packages we need for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7daa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd97c39",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Simple Linear Regression: Iris Dataset\n",
    "\n",
    "The iris dataset contains information about the sepal/petal width/length from 150 Iris flowers. We will use this data to build some regression models.\n",
    "\n",
    "![title](iris.jpeg)\n",
    "\n",
    "This first cell will read in the data we need for our first example of simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_data = pd.DataFrame(iris['data'])\n",
    "iris_data.columns = iris['feature_names']\n",
    "iris_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dccb1",
   "metadata": {},
   "source": [
    "In simple linear regression, we are fitting a \"line of best fit\" to the data which we can then use to make predictions. Let's see if we can predict the sepal length of an irish from the sepal width. Firstly let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65598c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iris_data['petal length (cm)'], iris_data['sepal length (cm)'])\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.xlabel('petal length (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225dfc3",
   "metadata": {},
   "source": [
    "There does seem to be some correlation here, so let's try and build a model. Now whilst we will be using the phrase \"Linear Model\", if this sounds a bit intimidating, just think of it as a line of best fit - this is fundamentally what linear models are doing.\n",
    "\n",
    "Now recall the form of a line\n",
    "$$y = ax + b$$\n",
    "we call $a$ the gradient, and $b$ the y-intercept. Remember\n",
    "\n",
    "* $a$: the gradient tells you how far you need to move up, for every one unit you move across\n",
    "* $b$: the y-intercept tells you where you would cross the y-axis (when x = 0)\n",
    "\n",
    "Now let's fit and plot our linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(fit_intercept=True)\n",
    "x = np.array(iris_data['petal length (cm)']).reshape((-1, 1))\n",
    "y = np.array(iris_data['sepal length (cm)'])\n",
    "mod = reg.fit(x,y)\n",
    "print(f\"intercept: {mod.intercept_}\")\n",
    "print(f\"slope: {mod.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef95b7",
   "metadata": {},
   "source": [
    "So the equation of the line is \n",
    "\n",
    "$$y = 0.41x + 4.31$$ \n",
    "\n",
    "Which we can plot on our graph from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iris_data['petal length (cm)'], iris_data['sepal length (cm)'])\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.xlabel('petal length (cm)')\n",
    "xmin = min(iris_data['petal length (cm)'])-1\n",
    "xmax = max(iris_data['petal length (cm)'])+1\n",
    "ymin = mod.intercept_ + mod.coef_[0]*xmin\n",
    "ymax = mod.intercept_ + mod.coef_[0]*xmax\n",
    "plt.plot([xmin, xmax], [ymin, ymax], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fca2f",
   "metadata": {},
   "source": [
    "Given a new petal length measurement, say 3cm, we can use our model to get the expected sepal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = mod.predict(np.array([3]).reshape(-1, 1))[0]\n",
    "print(\"When the petal length is 3cm, the expected sepal length is \" + str(np.round(new_value, 2)) + \"cm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e901fa",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "Linear regression actually allows us to use more than one predictor. Instead of just using petal length, we can also use petal length and sepal width! In an equation, it would be\n",
    "\n",
    "$$y = a + b \\times \\text{petal_length} + c \\times \\text{sepal_width} + d \\times \\text{petal_width}$$\n",
    "\n",
    "where $a$ is still the y-intercept, and this time $b, c$ and $d$ are the slopes of the petal length, sepal width and petal width respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c24388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(iris_data[['petal length (cm)', 'sepal width (cm)', 'petal width (cm)']]).reshape((-1, 3))\n",
    "y = np.array(iris_data['sepal length (cm)'])\n",
    "mod = reg.fit(x,y)\n",
    "\n",
    "print(f\"intercept: {mod.intercept_}\")\n",
    "print(f\"slope: {mod.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44591ca5",
   "metadata": {},
   "source": [
    "and similarly we can use this model to make predidctions given new data. Say we once again have an iris with a petal length of 3cm, but we now also know the sepal width is 3.5cm and the petal width is 2.4cm, we can use our model to calculate the expected petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mod.predict(np.array([3, 3.5, 2.4]).reshape(-1, 3))[0]\n",
    "print('The expected petal length is ' + str(np.round(pred, 2)) + 'cm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f280838",
   "metadata": {},
   "source": [
    "### Regression for classification - Logistic Regression\n",
    "\n",
    "In the iris example, we are looking at predicting a *continuous* response. In classification tasks, we look to model classified data - a good example of a classification task is the matching, where we classify a client control, and the classes are the Acin controls. We can use a variant of linear regression for binary classification tasks - binary meaning two classes. Let's see how this can work.\n",
    "\n",
    "\n",
    "Kaggle is a company that run Machine Learning competitions, and one of the most famous is predicting survival on the titanic, with over 14,000 thousand teams having submitted over 60,000 classification models. We will use this for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534b510",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('Titanic.csv')\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded320a6",
   "metadata": {},
   "source": [
    "First off, we will need to encode categorical variables, here the only one is Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars=['Sex']\n",
    "for var in cat_vars:\n",
    "    cat_list='var'+'_'+var\n",
    "    cat_list = pd.get_dummies(titanic[var], prefix=var)\n",
    "    titanic1=titanic.join(cat_list)\n",
    "    titanic=titanic1\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32db531",
   "metadata": {},
   "source": [
    "We will split this dataset into a training set and test set. We will use the training set to fit the parameters, and the test set to see how well the model performs on unseen data. We will use an 80-20 split, that is 80% of the data will be used for training, and 20% used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1296844",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(titanic['Survived'])\n",
    "train = random.sample(list(range(l)), int(l*0.8))\n",
    "train_set = titanic.iloc[train]\n",
    "test = [x for x in range(l) if x not in train]\n",
    "test_set = titanic.iloc[test]\n",
    "\n",
    "print(str(len(train)) + ' number of records in training set')\n",
    "print(str(len(test)) + ' number of records in test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425c105",
   "metadata": {},
   "source": [
    "Let's start by building a model that predicts whether the passenger survived or not based on only Sex and Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_mod = LogisticRegression()\n",
    "x_train = np.array(train_set[['Sex_female', 'Age']]).reshape((-1, 2))\n",
    "y_train = np.array(train_set['Survived'])\n",
    "fitted = log_mod.fit(x_train, y_train)\n",
    "\n",
    "print(f\"intercept: {fitted.intercept_}\")\n",
    "print(f\"slope: {fitted.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f60b81",
   "metadata": {},
   "source": [
    "We can actually use the slope values to understand their effect on Survival. Sex_female has a positive slope, meaning that if you are female, you are more likely to have survived (the positive slope pushes the output towards Survived=1). Age has a negative coefficient, meaning that if you were older, you were less likely to have survived (the negative means the higher the age, the more the output it pushed towards Survived=0).\n",
    "\n",
    "Now, time to find out if, by your age and sex, you would have survived the titanic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_Age = 20 # Enter your own age here\n",
    "your_IsFemale = 1 # Swap the 0 for a 1 if you are female\n",
    "\n",
    "surv_pred = fitted.predict(np.array([your_IsFemale, your_Age]).reshape(-1, 2))[0]\n",
    "\n",
    "if surv_pred == 1:\n",
    "    print('You Survived!')\n",
    "else:\n",
    "    print(\"Eek, our model doesn't think you'd have made it based on your age and Sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec59423",
   "metadata": {},
   "source": [
    "Now, we have to question here whether this model is \"good\" or not. This model in theory is a fairly sensible one - the \"women and childeren first\" policy was famously employed on the Titanic during the evacuation, with one of the crew interpreting the order as \"women and childeren only\" and released lifeboats with empty seats if there were no women and childeren nearby, refusing men waiting to evacuate entry.\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "Here we look at what percentage of the elements in the test set were correctly classified by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test_set[['Sex_female', 'Age']]).reshape((-1, 2))\n",
    "y_test = np.array(test_set['Survived'])\n",
    "\n",
    "score = fitted.score(x_test, y_test)\n",
    "print(str(np.round(score, 4)*100) + '% of test set elements were correctly predicted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40f14d",
   "metadata": {},
   "source": [
    "That doesn't sound too bad! Typically in tasks like this, we would use model selection techniques, using statistical approaches to determine which variables are most significant to include in the model, and discard those that are not significant. There are other model selection criteria we can use like Akike's Information Criterion and Bayesian Information Criterion - these look for models that best fit the data with the smallest number of parameters. We will not go into this today.\n",
    "\n",
    "\n",
    "One other thing we can use to evaluate model performance is the *ROC* curve (*R*eciever *O*perating *C*haracteristic). This shows us, as we move the threshold for classifying as 1, the change in true and false positive rate. The diagonal line shows us how a simple random guess would perform, and the more above that line the model is, the better it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec08a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = fitted.predict_proba(x_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr, label='Our Model')\n",
    "plt.plot([[0,0], [1,1]], color='red', label = 'Guess')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
